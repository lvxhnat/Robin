{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9137a52-8224-4c28-864b-c8dbc69c9ffa",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "Sometimes, it may be difficult to solve an equation analytically (through the use of fixed derivative rules), and might be easier to solve numerically (through the use of estimation or iterative finding of solutions). This notebook contains some of the methods (root/zero finding algorithms) I have learnt that can be used to solve univariate optimisation issues.  \n",
    "\n",
    "1. Bisection Method \n",
    "2. Newton's Method / Newton Rapshon Method\n",
    "3. Secant Method \n",
    "\n",
    "That is not to say methods of gradient descent cannot be used for the same purpose. See next notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee21167-c14c-42a9-9723-14c3ac18629c",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723b35e2-eb47-4094-9d1a-c4f22f979a0d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3029a-db6a-41a0-bb07-56cba9a1c132",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c47f62-9e07-4a4c-9a0a-a021cb78e464",
   "metadata": {},
   "source": [
    "#### Bisection Method\n",
    "\n",
    "The slowest algorithm, which uses iteration to find the local minimum using binary search. \n",
    "\n",
    "It works on the principle of the intermediate value theorem in calculus (See specifics below), narrowing the gap in between the positive and negative intervals (as we can see by the swapping of values a = m | b = m, until we close in on the answer to a certain acceptable error range $\\epsilon$).\n",
    "\n",
    "**Steps** \n",
    "1. We first find values of a and b, for which f(a) and f(b) are opposite signs. I do this either mathematically or through guess and check methods.\n",
    "2. Using the values of a and b, calculate the midpoint m = (a+b)/2\n",
    "3. Calculate f(m) \n",
    "4. We set an acceptable error range $\\epsilon$. \n",
    "5. If m - a is sufficiently small, i.e. m - a â‰¤ $\\epsilon$, then we stop iterating else we continue.\n",
    "6. Then, if f(a)f(m) < 0 we make b = m, else a = m\n",
    "7. Upon completing the iterations, our solution is the local minimum is m.\n",
    "\n",
    "**Specifics** \\\n",
    "In particular, the bisection method is built on a corollary of the Intermediate Value Theorem, Bolzano's Theorem, which states that: \\\n",
    "If $f:[a,b]$ is a continuous function such that $f(a)\\cdot f(b) < 0$, then there is $c \\in (a,b)$ such that $f(c) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b48c496-0f3c-4b87-b01d-acd9f64e1d8b",
   "metadata": {},
   "source": [
    "##### Let $g(x) = -12x + 3x^4 + 2x^6$\n",
    "##### Then $g'(x) = -12 + 12x^3 + 12x^5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "786a84d8-6e31-4345-9636-4bece25d9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 2\n",
    "\n",
    "gf_gx = lambda x: -12 + 12 * ( x ** 3 ) + 12 * ( x ** 5 )\n",
    "midpoint = lambda x, y: ( x + y ) / 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a55eb935-f0c2-4a24-b6ee-505d811ea977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-12, 468)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gf_gx_a = gf_gx(a)\n",
    "gf_gx_b = gf_gx(b)\n",
    "\n",
    "gf_gx_a, gf_gx_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68d4f7dd-91c8-4f89-b441-4dcf538c4a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_threshold = 0.0005\n",
    "m = midpoint(a,b)\n",
    "\n",
    "while (m - a > error_threshold): \n",
    "    if gf_gx(a) * gf_gx(m) < 0: \n",
    "        b = m\n",
    "    else: \n",
    "        a = m\n",
    "    m = midpoint(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bc78965-d5cc-4c2a-8fff-e498b3ad381b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83740234375"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738bf342-ed0d-4494-a7a7-94ef295ccd4e",
   "metadata": {},
   "source": [
    "#### Newton Method / Newton Raphson Method\n",
    "\n",
    "$\\large x_{n+1} = x_n -\\frac {f(x_n)} {f'(x_n)}$\n",
    "\n",
    "We perform this method iteratively, until we obtain a sufficiently precise value for which x is the local minimum. We approximate a starting value $x_n$ that is approximately close to the root. \n",
    "\n",
    "However, Newton's method requires that the derivative can be calculated directly which may not be easily obtainable or could be expensive to evaluate. In these situations, it may be appropriate to approximate the derivative by using the slope of a line through two nearby points on the function. Using this approximation would result in something like the secant method whose convergence is slower than that of Newton's method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede0fbc3-06d4-4697-804b-67624cad69fd",
   "metadata": {},
   "source": [
    "##### Let $f(x) = e^x+\\frac 1 2 x^2 - x$\n",
    "##### $f'(x) = e^x +x - 1$\n",
    "##### $f''(x) = e^x +1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de7aba94-2bfe-4179-8f7c-d364610c7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_x = lambda x: np.e ** x + 0.5 * x ** 2 - x\n",
    "fx_dx = lambda x: np.e ** x + x - 1\n",
    "fx2_dx2 = lambda x: np.e ** x + 1\n",
    "\n",
    "x = 1\n",
    "\n",
    "for i in range(1):\n",
    "    x -= fx_dx(x) / fx2_dx2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40e6c899-4296-4e55-ac6c-24ddcfcb6eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2689414213699951"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca538dba-84db-4438-b12c-ecb8688d4ebf",
   "metadata": {},
   "source": [
    "#### Secant Method\n",
    "\n",
    "Formula for this is:\n",
    "$ \\large x_{n + 1} = \\frac {f(x_{n})x_{n-1} - f(x_{n - 1})x_n} {f(x_n)-f(x_{n-1})}$\n",
    "\n",
    "Wikipedia sums up the derivation nicely: \n",
    "<img src = \"media/secant_derivation.png\"></img>\n",
    "\n",
    "We perform this iteration similar to that in the newton method. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
